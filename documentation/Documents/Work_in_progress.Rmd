---
title: "Work in progress"
author: "Jasper Velthoen"
date: "10/23/2019"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

This docuement contains the work that is in progress and code that still needs to be written:

\section{Work in Progress}
\begin{itemize}
  \item We now update each time with the linesearch in combination with the subsampling. This leads to the following problems:
  \begin{itemize}
    \item The subsampling makes sure that not every observation is sampled every time. As a result sometimes very high observations are undersampled several times leading to several updates until the observation is not in the domain of the estimated tail distribution.
    \item Second order derivatives can vary a lot in the leafnode sometimes leading to high and low values, the average value of these is then close to zero leading to a very large derivative.
  \end{itemize}
\end{itemize}

  We need to be very carefull with the updates, i.e. the learning rate. If this one is too large and a slight misspecification of the other parameters can leaf to a substantially worse model. Summary: everything is very sensitive to the tuning parameters, making a very simple CV very hard to perform.



\section{Programming goals}

\begin{itemize}
  \item Cross-validation procedure, This is tricky due to the fact that most tuning parameters work together, i.e. deeper leafnodes leads to better updates hence the learning rate or $B$ should go down to prevent overfitting.
  \item Out-of-Bag samples to check "on-the-fly" the correct stopping criterion (Does this work?)
  \item Add the possibility of getting quantile estimates from the predict function
  \item Possibility of adding a constant gamma by not splitting the tree
  \item Creating examples for different data purposes
\end{itemize}
